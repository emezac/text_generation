{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_trung.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mancap314/text_generation/blob/master/text_generation_trung.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "dHnNdaRBUOur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM Text Generation \n",
        "\n",
        "Based on this [blog post](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/) by Trung Tran\n",
        "\n",
        "## Step 1: Import Text Data\n",
        "\n",
        "We just import *War and Peace* by Tolstoy (hopefully the machine can read it faster than I...)"
      ]
    },
    {
      "metadata": {
        "id": "INhlxQ15UGpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "546f80a4-121f-4487-a6ab-56d70441396a"
      },
      "cell_type": "code",
      "source": [
        "!wget https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-06-27 20:50:21--  https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt\r\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3258246 (3.1M) [text/plain]\n",
            "Saving to: ‘warpeace_input.txt’\n",
            "\n",
            "warpeace_input.txt  100%[===================>]   3.11M  15.0MB/s    in 0.2s    \n",
            "\n",
            "2018-06-27 20:50:22 (15.0 MB/s) - ‘warpeace_input.txt’ saved [3258246/3258246]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9USmR28WWQCQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Prepare the data\n",
        "\n",
        "First read the data and get the set of unique characters:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bv7fS45VWJT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "15920fbd-25c7-4ed6-a618-50d17d26a875"
      },
      "cell_type": "code",
      "source": [
        "DATA_PATH = 'warpeace_input.txt'\n",
        "\n",
        "data = open(DATA_PATH, 'r').read()\n",
        "chars = list(set(data)) #set: gets unique values\n",
        "VOCAB_SIZE = len(chars)\n",
        "\n",
        "print('chars:\\n{}\\n\\nVOCAB_SIZE: {}'.format(chars, VOCAB_SIZE))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chars:\n",
            "[' ', '!', 'J', 'b', 'ä', 'S', '7', 'r', '?', 'k', 'B', 'Y', '2', 'g', '9', 'D', 'x', 'j', 't', 'C', 'e', 'o', 'u', 'K', '4', 'é', 'm', 'l', '/', '.', '\\ufeff', 'a', 'X', 'N', 'Z', '8', '6', 'i', '5', ';', 'H', 'q', 'à', '3', 's', 'p', 'O', 'n', 'G', 'E', 'Q', 'y', 'w', 'I', '(', 'R', '\"', 'P', '\\n', ':', 'F', 'd', '=', ',', '0', '1', 'L', ')', '-', 'T', '*', 'U', \"'\", 'h', 'M', 'z', 'f', 'W', 'A', 'v', 'ê', 'c', 'V']\n",
            "\n",
            "VOCAB_SIZE: 83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DbfHLMh9YbyR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then map the characters to integer numbers (and vice-versa):"
      ]
    },
    {
      "metadata": {
        "id": "HAFy8yppYZ3t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx_to_char = {i: char for i, char in enumerate(chars)}\n",
        "char_to_idx = {char: i for i, char in enumerate(chars)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HF2rM7t2axv7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let prepare the ground for the feature data **X** and the target variable **y**:"
      ]
    },
    {
      "metadata": {
        "id": "_VHvTWrsbDVc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "SEQ_LENGTH = 60 #input sequence length\n",
        "N_FEATURES = VOCAB_SIZE #one hot encoding here, that's why, but deduplicated for clarity\n",
        "\n",
        "N_SEQ = int(np.floor((len(data) - 1) / SEQ_LENGTH))\n",
        "\n",
        "X = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))\n",
        "y = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))\n",
        "\n",
        "for i in range(N_SEQ):\n",
        "  X_sequence = data[i * SEQ_LENGTH: (i + 1) * SEQ_LENGTH]\n",
        "  X_sequence_ix = [char_to_idx[c] for c in X_sequence]\n",
        "  input_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
        "  for j in range(SEQ_LENGTH):\n",
        "    input_sequence[j][X_sequence_ix[j]] = 1. #one-hot encoding of the input characters\n",
        "  X[i] = input_sequence\n",
        "  \n",
        "  y_sequence = data[i * SEQ_LENGTH + 1: (i + 1) * SEQ_LENGTH + 1] #shifted by 1 to the right\n",
        "  y_sequence_ix = [char_to_idx[c] for c in y_sequence]\n",
        "  target_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
        "  for j in range(SEQ_LENGTH):\n",
        "    target_sequence[j][y_sequence_ix[j]] = 1. #one-hot encoding of the target characters\n",
        "  y[i] = target_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XG-vW-bgtCAl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now define the model:"
      ]
    },
    {
      "metadata": {
        "id": "pQr8giR1tHd-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, TimeDistributed, Dense, Activation\n",
        "\n",
        "# constant parameter for the model\n",
        "HIDDEN_DIM = 700 #size of each hidden layer, \"each layer has 700 hidden states\"\n",
        "LAYER_NUM = 2 #number of hidden layers, how much were used?\n",
        "NB_EPOCHS = 200 #max number of epochs to train, \"200 epochs\"\n",
        "BATCH_SIZE = 128 \n",
        "VALIDATION_SPLIT = 0.1 #proportion of the batch used for validation at each epoch\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(HIDDEN_DIM, \n",
        "               input_shape=(None, VOCAB_SIZE), \n",
        "               return_sequences=True,\n",
        "               dropout=0.3, #\"Dropout ratio 0.3 at the first LSTM layer\"\n",
        "               recurrent_dropout=0.3))\n",
        "for _ in range(LAYER_NUM - 1):\n",
        "  model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lNzdbd-uzEzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create function to generate text from the trained model:"
      ]
    },
    {
      "metadata": {
        "id": "AsKrhaXxzEbi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model, length):\n",
        "  ix = [np.random.randint(VOCAB_SIZE)]\n",
        "  y_char = [idx_to_char[ix[-1]]]\n",
        "  X = np.zeros((1, length, VOCAB_SIZE))\n",
        "  for i in range(length):\n",
        "    X[0, i, :][ix[-1]] = 1.\n",
        "    ix = np.argmax(model.predict(X[:, :i+1,:])[0], 1)\n",
        "    y_char.append(idx_to_char[ix[-1]])\n",
        "  return ''.join(y_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "53Nflk2NLhBU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
        "# callback to save the model if better\n",
        "filepath=\"tgt_model.hdf5\"\n",
        "save_model_cb = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "# callback to stop the training if no improvement\n",
        "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=0)\n",
        "# callback to generate text at epoch end\n",
        "class generateText(Callback):\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        print(generate_text(self.model, 200))\n",
        "        \n",
        "generate_text_cb = generateText()\n",
        "\n",
        "callbacks_list = [save_model_cb, early_stopping_cb, generate_text_cb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o30Ib34yQ9_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now run the model:"
      ]
    },
    {
      "metadata": {
        "id": "oyCBmGnoRAYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "09aa25a5-c46f-476a-e0c5-483e722e9325"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=NB_EPOCHS, callbacks=callbacks_list, validation_split=VALIDATION_SPLIT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 47943 samples, validate on 5327 samples\n",
            "Epoch 1/200\n",
            "47943/47943 [==============================] - 2590s 54ms/step - loss: 2.6027 - acc: 0.2761 - val_loss: 1.9721 - val_acc: 0.4162\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.41622, saving model to tgt_model.hdf5\n",
            "Rostov and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and an\n",
            "Epoch 2/200\n",
            " 9728/47943 [=====>........................] - ETA: 33:22 - loss: 2.1227 - acc: 0.3842"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10112/47943 [=====>........................] - ETA: 33:00 - loss: 2.1201 - acc: 0.3848"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}