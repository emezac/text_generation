{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_trung.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mancap314/text_generation/blob/master/text_generation_trung.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "dHnNdaRBUOur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM Text Generation \n",
        "\n",
        "Based on this [blog post](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/) by Trung Tran\n",
        "\n",
        "## Step 1: Import Text Data\n",
        "\n",
        "We just import *War and Peace* by Tolstoy (hopefully the machine can read it faster than I...)"
      ]
    },
    {
      "metadata": {
        "id": "INhlxQ15UGpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6402af0d-e282-45d3-b218-f7dd95698e0d"
      },
      "cell_type": "code",
      "source": [
        "!wget https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-06-28 00:12:26--  https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt\r\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\r\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3258246 (3.1M) [text/plain]\n",
            "Saving to: ‘warpeace_input.txt.2’\n",
            "\n",
            "warpeace_input.txt. 100%[===================>]   3.11M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-06-28 00:12:27 (20.8 MB/s) - ‘warpeace_input.txt.2’ saved [3258246/3258246]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9USmR28WWQCQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Prepare the data\n",
        "\n",
        "First read the data and get the set of unique characters:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bv7fS45VWJT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "acf9e512-8ab9-417c-9deb-601d67123f1d"
      },
      "cell_type": "code",
      "source": [
        "DATA_PATH = 'warpeace_input.txt'\n",
        "\n",
        "data = open(DATA_PATH, 'r').read()\n",
        "chars = list(set(data)) #set: gets unique values\n",
        "VOCAB_SIZE = len(chars)\n",
        "\n",
        "print('chars:\\n{}\\n\\nVOCAB_SIZE: {}'.format(chars, VOCAB_SIZE))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chars:\n",
            "['(', 'c', '!', 'Y', 'R', 'Q', 'r', '4', 'i', 'é', 'J', 'x', 't', 'v', ':', 'l', '-', 'e', '2', ' ', 'B', \"'\", 'N', 'E', 'z', 'C', '*', '\\n', 'à', 'U', 'ä', 'O', '1', 'k', 'D', '\\ufeff', 'Z', 'd', 'V', 'o', 'p', 'I', 'm', ')', 'A', ';', 'S', 'q', 'X', 'b', '7', '9', '/', 'y', 'L', 'P', 'a', 'g', 'H', '5', 's', 'f', ',', 'G', 'K', '.', '0', '?', 'M', 'ê', '3', '6', 'w', '=', '\"', 'j', 'h', 'W', 'T', 'u', '8', 'F', 'n']\n",
            "\n",
            "VOCAB_SIZE: 83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DbfHLMh9YbyR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then map the characters to integer numbers (and vice-versa):"
      ]
    },
    {
      "metadata": {
        "id": "HAFy8yppYZ3t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b9cddc02-c8a3-4dce-d548-61d846c65e4e"
      },
      "cell_type": "code",
      "source": [
        "idx_to_char = {i: char for i, char in enumerate(chars)}\n",
        "char_to_idx = {char: i for i, char in enumerate(chars)}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HF2rM7t2axv7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let prepare the ground for the feature data **X** and the target variable **y**:"
      ]
    },
    {
      "metadata": {
        "id": "_VHvTWrsbDVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d9c299dc-c672-4a82-faf5-32231e0805eb"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "SEQ_LENGTH = 60 #input sequence length\n",
        "N_FEATURES = VOCAB_SIZE #one hot encoding here, that's why, but deduplicated for clarity\n",
        "\n",
        "N_SEQ = int(np.floor((len(data) - 1) / SEQ_LENGTH))\n",
        "\n",
        "X = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))\n",
        "y = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))\n",
        "\n",
        "for i in range(N_SEQ):\n",
        "  X_sequence = data[i * SEQ_LENGTH: (i + 1) * SEQ_LENGTH]\n",
        "  X_sequence_ix = [char_to_idx[c] for c in X_sequence]\n",
        "  input_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
        "  for j in range(SEQ_LENGTH):\n",
        "    input_sequence[j][X_sequence_ix[j]] = 1. #one-hot encoding of the input characters\n",
        "  X[i] = input_sequence\n",
        "  \n",
        "  y_sequence = data[i * SEQ_LENGTH + 1: (i + 1) * SEQ_LENGTH + 1] #shifted by 1 to the right\n",
        "  y_sequence_ix = [char_to_idx[c] for c in y_sequence]\n",
        "  target_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
        "  for j in range(SEQ_LENGTH):\n",
        "    target_sequence[j][y_sequence_ix[j]] = 1. #one-hot encoding of the target characters\n",
        "  y[i] = target_sequence"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XG-vW-bgtCAl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: define the model:"
      ]
    },
    {
      "metadata": {
        "id": "pQr8giR1tHd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10694db6-b7c7-4513-8b62-4349dac7644a"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import CuDNNLSTM, TimeDistributed, Dense, Activation\n",
        "\n",
        "# constant parameter for the model\n",
        "HIDDEN_DIM = 700 #size of each hidden layer, \"each layer has 700 hidden states\"\n",
        "LAYER_NUM = 2 #number of hidden layers, how much were used?\n",
        "NB_EPOCHS = 200 #max number of epochs to train, \"200 epochs\"\n",
        "BATCH_SIZE = 128 \n",
        "VALIDATION_SPLIT = 0.1 #proportion of the batch used for validation at each epoch\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(HIDDEN_DIM, \n",
        "               input_shape=(None, VOCAB_SIZE), \n",
        "               return_sequences=True))\n",
        "for _ in range(LAYER_NUM - 1):\n",
        "  model.add(CuDNNLSTM(HIDDEN_DIM, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "lNzdbd-uzEzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create function to generate text from the trained model:"
      ]
    },
    {
      "metadata": {
        "id": "AsKrhaXxzEbi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "82b7af7b-fda2-45d1-90c2-aac943a15c47"
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model, length):\n",
        "  ix = [np.random.randint(VOCAB_SIZE)]\n",
        "  y_char = [idx_to_char[ix[-1]]]\n",
        "  X = np.zeros((1, length, VOCAB_SIZE))\n",
        "  for i in range(length):\n",
        "    X[0, i, :][ix[-1]] = 1.\n",
        "    ix = np.argmax(model.predict(X[:, :i+1,:])[0], 1)\n",
        "    y_char.append(idx_to_char[ix[-1]])\n",
        "  return ''.join(y_char)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bgdq4ljMvmyZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define callbacks:"
      ]
    },
    {
      "metadata": {
        "id": "53Nflk2NLhBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "62b91d01-ac0a-4eb6-89cc-964fbe3539c4"
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
        "# callback to save the model if better\n",
        "filepath=\"tgt_model.hdf5\"\n",
        "save_model_cb = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "# callback to stop the training if no improvement\n",
        "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=0)\n",
        "# callback to generate text at epoch end\n",
        "class generateText(Callback):\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        print(generate_text(self.model, 100))\n",
        "        \n",
        "generate_text_cb = generateText()\n",
        "\n",
        "callbacks_list = [save_model_cb, early_stopping_cb, generate_text_cb]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o30Ib34yQ9_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: run the model:"
      ]
    },
    {
      "metadata": {
        "id": "oyCBmGnoRAYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "07539a74-90c2-498a-8693-42c43b349be6"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=NB_EPOCHS, callbacks=callbacks_list, validation_split=VALIDATION_SPLIT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 47943 samples, validate on 5327 samples\n",
            "Epoch 1/200\n",
            "47943/47943 [==============================] - 80s 2ms/step - loss: 2.5003 - acc: 0.3016 - val_loss: 1.9147 - val_acc: 0.4395\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.43945, saving model to tgt_model.hdf5\n",
            "-\n",
            "the stold the stold the stold the stold the stold the stold the stold the stold the stold the stold\n",
            "Epoch 2/200\n",
            " 7936/47943 [===>..........................] - ETA: 1:02 - loss: 1.8448 - acc: 0.4568"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "47943/47943 [==============================] - 78s 2ms/step - loss: 1.6546 - acc: 0.5094 - val_loss: 1.5094 - val_acc: 0.5463\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.43945 to 0.54632, saving model to tgt_model.hdf5\n",
            "5 the first the first the first the first the first the first the first the first the first the first\n",
            "Epoch 3/200\n",
            "13312/47943 [=======>......................] - ETA: 54s - loss: 1.4413 - acc: 0.5682"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}